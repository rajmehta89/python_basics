ğŸ§  What is Word2Vec?
Word2Vec is a technique that helps a computer understand what words mean by turning them into numbers.

 Why use it?
Because computers donâ€™t understand words, they only understand numbers.
Word2Vec helps by converting each word into a vector (a list of numbers), where:

Similar words have similar vectors

Example: "king" and "queen" will be close together in the vector space.

ğŸ” How does it work?
It learns from text (like books or articles) using one of two methods:

CBOW (Continuous Bag of Words)
Predicts the current word from nearby words.
Example: "The ___ barks" â†’ guesses "dog".

Skip-gram
Predicts nearby words from the current word.
Example: "dog" â†’ predicts "The", "barks"
 In short:
Word2Vec = turns words into smart numbers so machines can understand and work with language.


ğŸŒŸ Imagine a sentence:
"The cat sits on the mat"

ğŸŸ© 1. CBOW (Continuous Bag of Words)
ğŸ“Œ Goal:
Guess the middle word using the words around it.

ğŸ§  Think like this:

If you know the words around a blank, can you guess the missing word?

Example:

Input: ["The", "sits", "on"]

Output: "cat"

ğŸ“¦ So CBOW takes context words â†’ predicts center word

ğŸŸ¨ 2. Skip-gram
ğŸ“Œ Goal:
Guess the surrounding words using the center word.

ğŸ§  Think like this:

If you know one word, can you guess the words around it?


ğŸ¯ Summary (like a game):
CBOW: "Given neighbors, guess the center word"

Skip-gram: "Given center word, guess the neighbors"


